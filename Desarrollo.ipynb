{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas:\n",
    "* [a](#a)\n",
    "* [b](#b)\n",
    "* [c](#c)\n",
    "* [d](#d)\n",
    "* [e](#e)\n",
    "* [f](#f)\n",
    "* [g](#g)\n",
    "* [h](#h)\n",
    "* [i](#i)\n",
    "* [j](#j)\n",
    "* [k](#k)\n",
    "* [l](#l)\n",
    "* [m](#m)\n",
    "* [n](#n)\n",
    "* [o](#o)\n",
    "* [p](#p)\n",
    "* [q](#p)\n",
    "* [r](#p)\n",
    "* [s](#s)\n",
    "* [t](#t)\n",
    "* [u](#u)\n",
    "* [v](#v)\n",
    "* [Actividad 2](#Actividad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ftr = open(\"train_data.csv\", \"r\",  encoding=\"ISO-8859-1\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "df_train = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "df_train['Sentiment'] = (pd.to_numeric(df_train['Sentiment'])+1)/2 # 0 o 1\n",
    "\n",
    "fts = open(\"test_data.csv\", \"r\",  encoding=\"ISO-8859-1\")\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "df_test = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "df_test['Sentiment'] = (pd.to_numeric(df_test['Sentiment'])+1)/2 # 0 o 1\n",
    "\n",
    "df_train_text = df_train.Text\n",
    "df_test_text = df_test.Text\n",
    "labels_train = df_train.Sentiment.values\n",
    "labels_test = df_test.Sentiment.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)<a class=\"anchor\" id=\"a\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3554 entries, 0 to 3553\n",
      "Data columns (total 2 columns):\n",
      "Sentiment    3554 non-null float64\n",
      "Text         3554 non-null object\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 55.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>everything's serious , poetic , earnest and --...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>narratively , trouble every day is a plodding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a truly wonderful tale combined with stunning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>jason patric and ray liotta make for one splen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>haneke keeps us at arm's length . guided more ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0        0.0  everything's serious , poetic , earnest and --...\n",
       "1        0.0  narratively , trouble every day is a plodding ...\n",
       "2        1.0  a truly wonderful tale combined with stunning ...\n",
       "3        1.0  jason patric and ray liotta make for one splen...\n",
       "4        0.0  haneke keeps us at arm's length . guided more ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "suma = 0\n",
    "i = 0\n",
    "for sentence in df_train['Text']:\n",
    "    if i == 0:\n",
    "        minimo = len(sentence)\n",
    "        maximo = len(sentence)\n",
    "    else:\n",
    "        if minimo>len(sentence):\n",
    "            minimo = len(sentence)\n",
    "            min_sent = sentence\n",
    "        if maximo<len(sentence):\n",
    "            maximo = len(sentence)\n",
    "            max_sent = sentence\n",
    "    i+=1\n",
    "    suma += len(sentence)\n",
    "avg_len = suma/len(df_train['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114.70709060213844, 7, 267, 'crummy\\n')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(avg_len, minimo, maximo, min_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>screenwriter dan schneider and director shawn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>home alone goes hollywood , a funny premise un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>seldom has a movie so closely matched the spir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>less dizzying than just dizzy , the jaunt is p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>an ultra-low-budget indie debut that smacks mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0        0.0  screenwriter dan schneider and director shawn ...\n",
       "1        0.0  home alone goes hollywood , a funny premise un...\n",
       "2        1.0  seldom has a movie so closely matched the spir...\n",
       "3        0.0  less dizzying than just dizzy , the jaunt is p...\n",
       "4        0.0  an ultra-low-budget indie debut that smacks mo..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "suma = 0\n",
    "i = 0\n",
    "for sentence in df_test['Text']:\n",
    "    if i == 0:\n",
    "        minimo = len(sentence)\n",
    "        maximo = len(sentence)\n",
    "    else:\n",
    "        if minimo>len(sentence):\n",
    "            minimo = len(sentence)\n",
    "            min_sent = sentence\n",
    "        if maximo<len(sentence):\n",
    "            maximo = len(sentence)\n",
    "            max_sent = sentence\n",
    "    i+=1\n",
    "    suma += len(sentence)\n",
    "avg_len = suma/len(df_train['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116.4679234665166, 9, 268, 'horrible\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(avg_len, minimo, maximo, min_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un mejor analisis preliminar seria de utilidad la eliminacion de las stopwords a la hora de poder determinar los largos maximo, minimo y el promedio del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)<a class=\"anchor\" id=\"b\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train_text, df_val_text, labels_train, labels_val  = train_test_split(df_train, labels_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2843, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(711, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c)<a class=\"anchor\" id=\"c\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "def base_word(word):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    return wordlemmatizer.lemmatize(word) \n",
    "def word_extractor(text):\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text) #substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ base_word(word.lower()) for word in word_tokenize(text) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords: #delete stopwords\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "... #try yourself\n",
    "word_extractor(\"I love to eat cake\")\n",
    "word_extractor(\"I love eating cake\")\n",
    "word_extractor(\"I loved eating the cake\")\n",
    "word_extractor(\"I do not love eating cake\")\n",
    "word_extractor(\"I don't love eating cake\")\n",
    "... #try yourself\n",
    "texts_train = [word_extractor(text[1]) for text in df_train_text.to_numpy()]\n",
    "texts_val = [word_extractor(text[1]) for text in df_val_text.to_numpy()]\n",
    "texts_test = [word_extractor(text[1]) for text in df_test.to_numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La importancia del preprocesamiento en el dominio del lenguaje natural radica en disminuir las palabras que si bien aparecen con mas frecuencia son las menos significantes para realizar analisis (las stopwords) para asi reducir ruido y tiempo de ejecucion. Tambien es importante el proceso de reducir las palabras a su origen con stemming o lematizacion para evitar analizar palabras cuya semantica es identica pero estan escritas diferentes producto de una conjugacion. En este caso se utiliza lematizacion ya que presenta mejores resultados que stemming al realizar el proceso considerando el contexto en el cual se encuentra la palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d)<a class=\"anchor\" id=\"d\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary=False) #TF representation\n",
    "vectorizer.fit(texts_train)\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "... #transform val and test\n",
    "\n",
    "\n",
    "vectorizer2 = CountVectorizer(ngram_range=(1, 1), binary=False)\n",
    "vectorizer2.fit(texts_val)\n",
    "features_val = vectorizer.transform(texts_val)\n",
    "\n",
    "vectorizer3 = CountVectorizer(ngram_range=(1, 1), binary=False)\n",
    "vectorizer3.fit(texts_test)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "\n",
    "vocab2 = vectorizer.get_feature_names()\n",
    "dist2=list(np.array(features_val.sum(axis=0)).reshape(-1,))\n",
    "\n",
    "vocab3 = vectorizer.get_feature_names()\n",
    "dist3=list(np.array(features_test.sum(axis=0)).reshape(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e)<a class=\"anchor\" id=\"e\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "for s in labels_train:\n",
    "    if s == 1:\n",
    "        sentiment.append(\"Positivo\")\n",
    "    else:\n",
    "        sentiment.append(\"Negativo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD #aka LSA\n",
    "import matplotlib.pyplot as plt\n",
    "model = TruncatedSVD(n_components=2)\n",
    "model.fit(features_train)\n",
    "x_plot = model.transform(features_train)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(x_plot[:,0], x_plot[:,1], c=labels_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1786], dtype=int64), array([0], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(x_plot == x_plot.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.95011268, 1.75215041])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_plot[1786]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positivo'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment[1786]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amarillo Positivo, Morado Negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las agrupaciones que se forman parecen contener una cantidad equitativa de positivo y negativo. Estas agrupaciones corresponden a las tematicas de las distintas criticas, pudiendo ser que cada agrupacion sea una pelicula distinta o que hablen sobre caracteristicas de las peliculas como la musica, los actores, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f)<a class=\"anchor\" id=\"f\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1543e70181444478887188a03d55b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Css', options=(0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interact_doLOGIT(Css)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "\n",
    "def do_LOGIT(x,y,xv,yv, param):\n",
    "    print(\"Param C= \",param)\n",
    "    model= LogisticRegression(penalty = 'l2')\n",
    "    model.set_params(C=param)\n",
    "    model.fit(x,y)\n",
    "    train_acc = model.score(x,y)\n",
    "    test_acc = model.score(xv,yv)\n",
    "    return model, train_acc, test_acc\n",
    "#Cs = [10**i for i in np.arange(-4,4)]\n",
    "Cs = np.power(np.repeat(10., 8),np.arange(-4,4))\n",
    "def interact_doLOGIT(Css):\n",
    "    model, train_acc, test_acc = do_LOGIT(features_train,labels_train,features_val,labels_val, param= Css )\n",
    "    print(\"train acc: \",train_acc)\n",
    "    print(\"val acc: \",test_acc)\n",
    "    \n",
    "    #Train\n",
    "    lista_train = model.predict(features_train)\n",
    "    #\n",
    "    y_true_train = list(map(str,lista_train))\n",
    "    y_pred_train = list(map(str,labels_train))\n",
    "    data = confusion_matrix(y_true_train, y_pred_train)\n",
    "    df_cm = pd.DataFrame(data, columns=np.unique(y_true_train), index = np.unique(y_true_train))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted Train'\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.set(font_scale=1.2)#for label size\n",
    "    sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14})# font size\n",
    "    \n",
    "    #Val\n",
    "    lista_val = model.predict(features_val)\n",
    "    #\n",
    "    y_true_val = list(map(str,lista_val))\n",
    "    y_pred_val = list(map(str,labels_val))\n",
    "    data = confusion_matrix(y_true_val, y_pred_val)\n",
    "    df_cm = pd.DataFrame(data, columns=np.unique(y_true_val), index = np.unique(y_true_val))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted Val'\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.set(font_scale=1.2)#for label size\n",
    "    sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14})# font size\n",
    "    \n",
    "interact(interact_doLOGIT, Css=Cs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones de lo que se observa de los heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g)<a class=\"anchor\" id=\"g\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842cb300e6084c179bf8425985e2a2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Css', options=(0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interact_doSVM(Css, kernels)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC as SVM #SVC is for classification\n",
    "def do_SVM(x,y,xv,yv, param, kernel='linear'):\n",
    "    C = param\n",
    "    print(\"Param C= \",C, 'Kernel= ', kernel)\n",
    "    model= SVM()\n",
    "    model.set_params(C=C,kernel=kernel) #try rbf and linear at least\n",
    "    model.fit(x,y)\n",
    "    train_acc = model.score(x,y)\n",
    "    test_acc = model.score(xv,yv)\n",
    "    return model, train_acc, test_acc\n",
    "Cs = np.power(np.repeat(10., 8),np.arange(-4,4))\n",
    "\n",
    "def interact_doSVM(Css, kernels):\n",
    "    model, train_acc, test_acc = do_SVM(features_train, labels_train, features_val, labels_val, param= Css, kernel=kernels)\n",
    "    print(\"train acc: \",train_acc)\n",
    "    print(\"val acc: \",test_acc)\n",
    "    \n",
    "    #Train\n",
    "    lista_train = model.predict(features_train)\n",
    "    #\n",
    "    y_true_train = list(map(str,lista_train))\n",
    "    y_pred_train = list(map(str,labels_train))\n",
    "    data = confusion_matrix(y_true_train, y_pred_train)\n",
    "    df_cm = pd.DataFrame(data, columns=np.unique(y_true_train), index = np.unique(y_true_train))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted Train'\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.set(font_scale=1.2)#for label size\n",
    "    sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14})# font size\n",
    "    \n",
    "    #Val\n",
    "    lista_val = model.predict(features_val)\n",
    "    #\n",
    "    y_true_val = list(map(str,lista_val))\n",
    "    y_pred_val = list(map(str,labels_val))\n",
    "    data = confusion_matrix(y_true_val, y_pred_val)\n",
    "    df_cm = pd.DataFrame(data, columns=np.unique(y_true_val), index = np.unique(y_true_val))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted Val'\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.set(font_scale=1.2)#for label size\n",
    "    sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14})# font size\n",
    "\n",
    "interact(interact_doSVM, Css=Cs, kernels = ['linear', 'rbf'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con parametro de regularizacion C menor a 0.01 con kernel lineal, la SVM predice que todos los comentarios son negativos. Asi mismo ocurre con al utilizar el kernel rbf solo que para este caso el valor de C no puede ser menor a 10, pero aun con este valor, el accuracy de la SVM solo llega a ser del 50% por lo que es recomendable utilizar un valor mayor. Para el caso del kernel lineal con C igual a 0.01 se obtienen mejores resultados que rbf con 10, pero aun asi mejorables. Al aumentar el valor de C el accuracy para el training set mejora llegando a ser igual a 1 para valores de C > 1 pero para el caso de test set el valor llega a su maximo (69.2%) para C = 0.1 y con valores mayores va descendiendo hasta 68.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h)<a class=\"anchor\" id=\"h\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#k_range = range(1, features_train.shape[0])\n",
    "#scores = []\n",
    "#for k in k_range:\n",
    "#    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "#    knn.fit(features_train, labels_train)\n",
    "#    scores.append(knn.score(features_val, labels_val))\n",
    "#plt.figure(figsize = (16,9))\n",
    "#plt.xlabel('k')\n",
    "#plt.ylabel('accuracy')\n",
    "#plt.scatter(k_range, scores)\n",
    "#plt.xticks(np.arange(1, features_train.shape[0], 100)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores.index(max(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba29e6a96b04cb481d6fb15191de71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1422, description='Kss', max=2843, min=1), Output()), _dom_classes=('wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interact_doKNN(Kss)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def do_KNN(x,y,xv,yv, param):\n",
    "    model = KNeighborsClassifier()\n",
    "    print(\"Param K= \",param)\n",
    "    model.set_params(n_neighbors=param)\n",
    "    model.fit(x,y)\n",
    "    train_acc = model.score(x,y)\n",
    "    test_acc = model.score(xv,yv)\n",
    "    return model, train_acc, test_acc\n",
    "\n",
    "def interact_doKNN(Kss):\n",
    "    model, train_acc, test_acc = do_KNN(features_train, labels_train, features_val, labels_val, param= Kss)\n",
    "    print(\"train acc: \",train_acc)\n",
    "    print(\"val acc: \",test_acc)\n",
    "    \n",
    "    #Train\n",
    "    lista_train = model.predict(features_train)\n",
    "    #\n",
    "    y_true_train = list(map(str,lista_train))\n",
    "    y_pred_train = list(map(str,labels_train))\n",
    "    data = confusion_matrix(y_true_train, y_pred_train)\n",
    "    df_cm = pd.DataFrame(data, columns=np.unique(y_true_train), index = np.unique(y_true_train))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted Train'\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.set(font_scale=1.2)#for label size\n",
    "    sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14})# font size\n",
    "    \n",
    "    #Val\n",
    "    lista_val = model.predict(features_val)\n",
    "    #\n",
    "    y_true_val = list(map(str,lista_val))\n",
    "    y_pred_val = list(map(str,labels_val))\n",
    "    data = confusion_matrix(y_true_val, y_pred_val)\n",
    "    df_cm = pd.DataFrame(data, columns=np.unique(y_true_val), index = np.unique(y_true_val))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted Val'\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.set(font_scale=1.2)#for label size\n",
    "    sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14})#\n",
    "\n",
    "Ks = np.arange(1, features_train.shape[0], 10)    \n",
    "interact(interact_doKNN, Kss=(1, features_train.shape[0], 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del valor de k-neighbors utilizados es posible decir que para determinar el sentimiento de una review se necesitan un quinto de los datos que presenten menor disancia (probablemente cosenoidal debido a la dimensionalidad) respecto a la review que se desea predecir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i)<a class=\"anchor\" id=\"i\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755fb962d6d240fb94c8f0688cd3e845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Depth', options=(1, 501, 1001, 1501, 2001, 2501, 3001, 3501, 4001,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interact_doTree(Depth, Samples)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "def do_Tree(x,y,xv,yv, param_d=None, param_m=2):\n",
    "    model= Tree()\n",
    "    print(\"Param Max-D= \",param_d, 'Min-samples-S= ', param_m)\n",
    "    model.set_params(max_depth=param_d, min_samples_split=param_m) \n",
    "    model.fit(x,y)\n",
    "    train_acc = model.score(x,y)\n",
    "    test_acc = model.score(xv,yv)\n",
    "    return model, train_acc, test_acc\n",
    "\n",
    "def interact_doTree(Depth, Samples):\n",
    "    model, train_acc, test_acc = do_Tree(features_train, labels_train, features_val, labels_val, param_d = Depth ,param_m = Samples)\n",
    "    print(\"train acc: \",train_acc)\n",
    "    print(\"val acc: \",test_acc)\n",
    "    \n",
    "    #Train\n",
    "    lista_train = model.predict(features_train)\n",
    "    #\n",
    "    y_true_train = list(map(str,lista_train))\n",
    "    y_pred_train = list(map(str,labels_train))\n",
    "    data = confusion_matrix(y_true_train, y_pred_train)\n",
    "    df_cm = pd.DataFrame(data, columns=np.unique(y_true_train), index = np.unique(y_true_train))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted Train'\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.set(font_scale=1.2)#for label size\n",
    "    sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14})# font size\n",
    "    \n",
    "    #Val\n",
    "    lista_val = model.predict(features_val)\n",
    "    #\n",
    "    y_true_val = list(map(str,lista_val))\n",
    "    y_pred_val = list(map(str,labels_val))\n",
    "    data = confusion_matrix(y_true_val, y_pred_val)\n",
    "    df_cm = pd.DataFrame(data, columns=np.unique(y_true_val), index = np.unique(y_true_val))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted Val'\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.set(font_scale=1.2)#for label size\n",
    "    sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14})#\n",
    "\n",
    "Depths = np.arange(1, features_train.shape[1], 500 ) #choose steps\n",
    "SamplesS = np.arange(2, features_train.shape[0] , 500 ) #choose steps\n",
    "interact(interact_doTree, Depth = Depths, Samples = SamplesS )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar que al realizar variaciones a la variable Samples el accuracy del arbol disminuye, esto puede ser debido a que al restringir la cantidad de muestras requeridas para seguir subdividiendo los nodos muchas quedan mal clasificadas. Para las opciones presentadas se obtuvo que con un Samples = 2 se obtenia el mejor resultado, pero puede ser que exista un mejor valor que se encuentre entre los valores que se presentan en el dropdown.\n",
    "\n",
    "Por otro lado en la variable Depth, no se observa cambios significativos a la hora de aumentar la profundidad del arbol, aunque es importante destacar que al aumentar el valor se producia una disminucion de la accuracy, como lo fue para Depth = 5501 cuya accuracy era peor que para Depth = 501 en el validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j)<a class=\"anchor\" id=\"j\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-25-3927bbedc164>\", line 1, in <module>\n",
      "    from keras.models import Sequential\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\n",
      "    from .load_backend import epsilon\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Program Files\\Python36\\lib\\inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Program Files\\Python36\\lib\\inspect.py\", line 730, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 936, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 655, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.ops.gen_audio_ops import decode_wav\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-25-3927bbedc164>\", line 1, in <module>\n",
      "    from keras.models import Sequential\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\n",
      "    from .load_backend import epsilon\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-25-3927bbedc164>\", line 1, in <module>\n",
      "    from keras.models import Sequential\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\n",
      "    from .load_backend import epsilon\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2043, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1385, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in structured_traceback\n",
      "    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\n",
      "TypeError: must be str, not list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Program Files\\Python36\\lib\\inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Program Files\\Python36\\lib\\inspect.py\", line 730, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 936, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 655, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.ops.gen_audio_ops import decode_wav\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-25-3927bbedc164>\", line 1, in <module>\n",
      "    from keras.models import Sequential\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\n",
      "    from .load_backend import epsilon\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2043, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1385, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in structured_traceback\n",
      "    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\n",
      "TypeError: must be str, not list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2039\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2040\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2041\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ImportError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[0;32m   3341\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3342\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3343\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3344\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3345\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2041\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2042\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 2043\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   2044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1385\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1286\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1288\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1289\u001b[0m             )\n\u001b[0;32m   1290\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Minimal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m             \u001b[0mformatted_exceptions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_chained_exception_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m             \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be str, not list"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "def do_ANN(x,y, xv,yv, param):\n",
    "    print(\"Neuron hidden = \",param)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=param, input_dim=x.shape[1], activation=\"sigmoid\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=SGD(lr=0.1), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit(x, y, epochs=25, batch_size=128, verbose=0)\n",
    "    train_acc = model.evaluate(x,y, verbose=0)[1] #in position 0 is the loss\n",
    "    test_acc = model.evaluate(xv,yv, verbose=0)[1]\n",
    "    return model, train_acc, test_acc\n",
    "\n",
    "def interact_doANN(paramm):\n",
    "    train_list = []\n",
    "    test_list=[]\n",
    "    for i in paramm:\n",
    "      model, train_acc, test_acc = do_ANN(features_train, labels_train, features_val, labels_val, param = i)\n",
    "      print(\"train acc: \",train_acc)\n",
    "      print(\"val acc: \",test_acc)\n",
    "      train_list.append(train_acc)\n",
    "      test_list.append(test_acc)\n",
    "    plt.figure(figsize = (16,9))\n",
    "    plt.xlabel('Neurons')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.scatter(paramm, train_list, label= \"train\")\n",
    "    plt.scatter(paramm, test_list, label = \"val\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "      \n",
    "    \n",
    "N_h = [2**i for i in range(1,10)]\n",
    "neurons = np.asarray(N_h)\n",
    "interact_doANN(N_h)    \n",
    "#interact(interact_doANN, paramm = (2**i for i in range(1,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resultado_NN.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a problemas con Tensorflow este item fue desarrollado en Google Colab y luego se copio el codigo junto al resultado obtenido.\n",
    "\n",
    "Del grafico es posible observar que en el training set la red logra alcanzar como mejor accuracy un 70% al utilizar 16 neuronas en la capa oculta y a medida que se iban aumentando el accuracy comenzó a disminuir convergiendo a 50%, comparable a lanzar una moneda.\n",
    "\n",
    "Por otro lado para el validation set se obtiene que el mayor valor de accuracy tambien se obtiene para 16 neuronas pero este es considerablemente mas bajo que el de training ya que solo llega a sobrepasar ligeramente el 60%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k)<a class=\"anchor\" id=\"k\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De todos los modelos utilizados el que mejor desempeño presento en el conjunto de validacion corresponde al modelo de regresion logistica con parametro de regularizacion \"C\" igual a 1 donde se presenta un accuracy de 71%, por lo que se procede a revisar su desempeño con el test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param C=  1.0\n"
     ]
    }
   ],
   "source": [
    "model, train_acc, test_acc = do_LOGIT(features_train,labels_train,features_val,labels_val, param= 1.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7146876758581879"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar que en el test set tambien se obtiene un valor de accuracy similar al del validation set. Este valor resulta bastante positivo teniendo en cuenta que se esperaba obtener un valor mas bajo que en el caso del validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l)<a class=\"anchor\" id=\"l\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palabras con igual intensidad: 547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6359032076533483"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "def vader_predict(sentences): \n",
    "    sid_obj = SentimentIntensityAnalyzer() \n",
    "    sent_v = []\n",
    "    count = 0 #para contar palabras iguales\n",
    "    for text in sentences:\n",
    "        sentiment_dict = sid_obj.polarity_scores(text) \n",
    "        if sentiment_dict[\"pos\"] > sentiment_dict[\"neg\"]: #based on scores\n",
    "            sent_v.append(1)\n",
    "        else:\n",
    "            if sentiment_dict[\"pos\"] == sentiment_dict[\"neg\"]:\n",
    "                  count = count + 1\n",
    "            sent_v.append(0)\n",
    "    print(\"palabras con igual intensidad: \" + str(count))\n",
    "    return np.asarray(sent_v)\n",
    "vader_pred_test = vader_predict(df_test_text)\n",
    "#vader_pred_test = vader_predict(text_test) #prueba sin stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels_test, vader_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que al utilizar el modelo Vader se obtiene un valor más bajo de accuracy que el obtenido utilizando la regresión logística. Esto puede deberse a que en el caso que la intensidad de la frase sea igualmente positiva que negativa, esta es catalogada como negativa por el codigo de la celda anterior, por lo que puede darse el caso que existan frases que deben ser positivas dentro de ese conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m)<a class=\"anchor\" id=\"m\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomando en cuenta que el modelo que presento mejor desempeño es el de Regresion Logistica, se procedera a seguir utilizando en este apartado dado que es un modelo que realiza predicciones probabilisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '100',\n",
       " '101',\n",
       " '105',\n",
       " '10th',\n",
       " '11',\n",
       " '110',\n",
       " '11th',\n",
       " '13',\n",
       " '13th',\n",
       " '14',\n",
       " '146',\n",
       " '15',\n",
       " '16',\n",
       " '163',\n",
       " '170',\n",
       " '18th',\n",
       " '19',\n",
       " '1915',\n",
       " '1934',\n",
       " '1938',\n",
       " '1940s',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1954',\n",
       " '1955',\n",
       " '1958',\n",
       " '1959',\n",
       " '1960s',\n",
       " '1972',\n",
       " '1975',\n",
       " '1978',\n",
       " '1979',\n",
       " '1980',\n",
       " '1992',\n",
       " '1995',\n",
       " '19th',\n",
       " '20',\n",
       " '2000',\n",
       " '2002',\n",
       " '20th',\n",
       " '21st',\n",
       " '22',\n",
       " '24',\n",
       " '25',\n",
       " '25s',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30s',\n",
       " '37',\n",
       " '3d',\n",
       " '40',\n",
       " '400',\n",
       " '401',\n",
       " '40s',\n",
       " '451',\n",
       " '48',\n",
       " '4ever',\n",
       " '50',\n",
       " '51',\n",
       " '53',\n",
       " '5ths',\n",
       " '60s',\n",
       " '65',\n",
       " '65th',\n",
       " '70s',\n",
       " '71',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '80',\n",
       " '800',\n",
       " '80s',\n",
       " '84',\n",
       " '85',\n",
       " '88',\n",
       " '8th',\n",
       " '90',\n",
       " '90s',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '99',\n",
       " 'aaliyah',\n",
       " 'abandon',\n",
       " 'abandono',\n",
       " 'abbreviated',\n",
       " 'abc',\n",
       " 'abel',\n",
       " 'abhorrent',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'ably',\n",
       " 'aboul',\n",
       " 'above',\n",
       " 'abrahams',\n",
       " 'abrasive',\n",
       " 'abroad',\n",
       " 'abruptly',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorbing',\n",
       " 'absorption',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'absurdist',\n",
       " 'absurdity',\n",
       " 'abuse',\n",
       " 'abysmally',\n",
       " 'acabamos',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'accent',\n",
       " 'accentuating',\n",
       " 'accept',\n",
       " 'accepting',\n",
       " 'accessible',\n",
       " 'accident',\n",
       " 'accompanies',\n",
       " 'accompanying',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'accomplishment',\n",
       " 'account',\n",
       " 'accountant',\n",
       " 'accumulated',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accuse',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achieves',\n",
       " 'achilles',\n",
       " 'achingly',\n",
       " 'achival',\n",
       " 'achronological',\n",
       " 'acid',\n",
       " 'acknowledges',\n",
       " 'acolyte',\n",
       " 'acquire',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actorliness',\n",
       " 'actorly',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adage',\n",
       " 'adam',\n",
       " 'adaptation',\n",
       " 'adapted',\n",
       " 'adapts',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addictive',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'address',\n",
       " 'adicional',\n",
       " 'administration',\n",
       " 'admirable',\n",
       " 'admirably',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admirer',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admittedly',\n",
       " 'adobo',\n",
       " 'adolescent',\n",
       " 'adoration',\n",
       " 'adored',\n",
       " 'adoring',\n",
       " 'adorned',\n",
       " 'adorns',\n",
       " 'adrenaline',\n",
       " 'adrenalized',\n",
       " 'adroit',\n",
       " 'adroitly',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'adventure',\n",
       " 'adventurous',\n",
       " 'adversity',\n",
       " 'advertised',\n",
       " 'advertisement',\n",
       " 'advice',\n",
       " 'advised',\n",
       " 'aerial',\n",
       " 'aesthetic',\n",
       " 'affability',\n",
       " 'affair',\n",
       " 'affecting',\n",
       " 'affection',\n",
       " 'affectionately',\n",
       " 'affinity',\n",
       " 'affirm',\n",
       " 'affirmational',\n",
       " 'affirming',\n",
       " 'affirms',\n",
       " 'affleck',\n",
       " 'affluence',\n",
       " 'afghan',\n",
       " 'afghani',\n",
       " 'afloat',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'afterschool',\n",
       " 'aftertaste',\n",
       " 'agape',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'agey',\n",
       " 'aggrandizing',\n",
       " 'aggravating',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'aging',\n",
       " 'ago',\n",
       " 'agonizing',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahh',\n",
       " 'ahola',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aids',\n",
       " 'aiello',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aiming',\n",
       " 'aimless',\n",
       " 'aimlessly',\n",
       " 'air',\n",
       " 'airborne',\n",
       " 'airhead',\n",
       " 'airless',\n",
       " 'aisle',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'alabama',\n",
       " 'alacrity',\n",
       " 'alain',\n",
       " 'album',\n",
       " 'alcatraz',\n",
       " 'aleck',\n",
       " 'alert',\n",
       " 'alexandre',\n",
       " 'ali',\n",
       " 'alias',\n",
       " 'alien',\n",
       " 'alienate',\n",
       " 'alienating',\n",
       " 'alienation',\n",
       " 'alike',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allegedly',\n",
       " 'allegiance',\n",
       " 'allegory',\n",
       " 'allen',\n",
       " 'allergy',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'ally',\n",
       " 'alma',\n",
       " 'almodovar',\n",
       " 'almost',\n",
       " 'aloft',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'aloof',\n",
       " 'already',\n",
       " 'also',\n",
       " 'alt',\n",
       " 'alternate',\n",
       " 'alternately',\n",
       " 'alternative',\n",
       " 'although',\n",
       " 'altman',\n",
       " 'always',\n",
       " 'amalgam',\n",
       " 'amalgamating',\n",
       " 'amaro',\n",
       " 'amateurish',\n",
       " 'amateurishness',\n",
       " 'amaze',\n",
       " 'amazement',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'ambience',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambition',\n",
       " 'ambitious',\n",
       " 'ambivalence',\n",
       " 'ambrose',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amiable',\n",
       " 'amid',\n",
       " 'among',\n",
       " 'amor',\n",
       " 'amoral',\n",
       " 'amorality',\n",
       " 'amorous',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'ample',\n",
       " 'amuse',\n",
       " 'amused',\n",
       " 'amusement',\n",
       " 'amuses',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'amélie',\n",
       " 'ana',\n",
       " 'anachronistic',\n",
       " 'anakin',\n",
       " 'analgesic',\n",
       " 'analysis',\n",
       " 'analytical',\n",
       " 'anarchic',\n",
       " 'anarchist',\n",
       " 'anchor',\n",
       " 'ancient',\n",
       " 'and',\n",
       " 'anderson',\n",
       " 'andrei',\n",
       " 'andrew',\n",
       " 'andy',\n",
       " 'anemic',\n",
       " 'anew',\n",
       " 'angel',\n",
       " 'angeles',\n",
       " 'angelique',\n",
       " 'angels',\n",
       " 'angle',\n",
       " 'angling',\n",
       " 'angry',\n",
       " 'angst',\n",
       " 'animal',\n",
       " 'animated',\n",
       " 'animation',\n",
       " 'anime',\n",
       " 'anna',\n",
       " 'annals',\n",
       " 'anne',\n",
       " 'annie',\n",
       " 'anniversary',\n",
       " 'annoying',\n",
       " 'annoyingly',\n",
       " 'anomie',\n",
       " 'anonymity',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'ante',\n",
       " 'anthony',\n",
       " 'anthropologically',\n",
       " 'anti',\n",
       " 'antiseptic',\n",
       " 'antler',\n",
       " 'antonia',\n",
       " 'antsy',\n",
       " 'antwone',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'ao',\n",
       " 'apart',\n",
       " 'ape',\n",
       " 'apenas',\n",
       " 'appalled',\n",
       " 'appalling',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'appease',\n",
       " 'appetizer',\n",
       " 'applaud',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'appointed',\n",
       " 'appreciate',\n",
       " 'appreciates',\n",
       " 'appreciative',\n",
       " 'approach',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'appropriated',\n",
       " 'appropriately',\n",
       " 'apt',\n",
       " 'apted',\n",
       " 'apuestas',\n",
       " 'aquatic',\n",
       " 'aquel',\n",
       " 'ararat',\n",
       " 'arbitrary',\n",
       " 'arch',\n",
       " 'archibald',\n",
       " 'architectural',\n",
       " 'architecture',\n",
       " 'archival',\n",
       " 'archive',\n",
       " 'ardently',\n",
       " 'arduous',\n",
       " 'are',\n",
       " 'area',\n",
       " 'argentine',\n",
       " 'argento',\n",
       " 'argues',\n",
       " 'argument',\n",
       " 'arise',\n",
       " 'arising',\n",
       " 'aristocrat',\n",
       " 'aristocratic',\n",
       " 'arithmetic',\n",
       " 'ark',\n",
       " 'arliss',\n",
       " 'arm',\n",
       " 'armenia',\n",
       " 'armenian',\n",
       " 'arnie',\n",
       " 'arnold',\n",
       " 'around',\n",
       " 'arrangement',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'arresting',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'arrogance',\n",
       " 'art',\n",
       " 'artefact',\n",
       " 'articulate',\n",
       " 'articulated',\n",
       " 'artifice',\n",
       " 'artificial',\n",
       " 'artificiality',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artistically',\n",
       " 'artistry',\n",
       " 'arts',\n",
       " 'artsy',\n",
       " 'as',\n",
       " 'asay',\n",
       " 'ascension',\n",
       " 'ascertain',\n",
       " 'ash',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspect',\n",
       " 'aspiration',\n",
       " 'aspiring',\n",
       " 'assassin',\n",
       " 'assassination',\n",
       " 'assaultive',\n",
       " 'assayas',\n",
       " 'assed',\n",
       " 'assembled',\n",
       " 'assertive',\n",
       " 'assess',\n",
       " 'asset',\n",
       " 'assign',\n",
       " 'assimilated',\n",
       " 'associate',\n",
       " 'associated',\n",
       " 'assume',\n",
       " 'assured',\n",
       " 'assuredly',\n",
       " 'astonishing',\n",
       " 'astonishingly',\n",
       " 'astounding',\n",
       " 'astray',\n",
       " 'astute',\n",
       " 'asylum',\n",
       " 'atacar',\n",
       " 'atacarse',\n",
       " 'atavistic',\n",
       " 'athlete',\n",
       " 'athletic',\n",
       " 'atmosphere',\n",
       " 'atmospheric',\n",
       " 'atmospherics',\n",
       " 'atonal',\n",
       " 'atop',\n",
       " 'atreve',\n",
       " 'atrocious',\n",
       " 'atrociously',\n",
       " 'attached',\n",
       " 'attachment',\n",
       " 'attack',\n",
       " 'attal',\n",
       " 'attempt',\n",
       " 'attention',\n",
       " 'attentive',\n",
       " 'attitude',\n",
       " 'attract',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'attuned',\n",
       " 'audacious',\n",
       " 'audacity',\n",
       " 'audiard',\n",
       " 'audience',\n",
       " 'augmentation',\n",
       " 'august',\n",
       " 'auschwitz',\n",
       " 'auspicious',\n",
       " 'aussie',\n",
       " 'austere',\n",
       " 'austerity',\n",
       " 'austin',\n",
       " 'australia',\n",
       " 'austrian',\n",
       " 'auteur',\n",
       " 'authentic',\n",
       " 'authentically',\n",
       " 'authenticate',\n",
       " 'authenticity',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'auto',\n",
       " 'autobiographical',\n",
       " 'autocritique',\n",
       " 'automated',\n",
       " 'automatically',\n",
       " 'available',\n",
       " 'avary',\n",
       " 'avenges',\n",
       " 'average',\n",
       " 'aversion',\n",
       " 'avert',\n",
       " 'avid',\n",
       " 'avoid',\n",
       " 'avoided',\n",
       " 'avoiding',\n",
       " 'avoids',\n",
       " 'awake',\n",
       " 'awakens',\n",
       " 'award',\n",
       " 'awarded',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awed',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awfully',\n",
       " 'awkward',\n",
       " 'awkwardly',\n",
       " 'ayala',\n",
       " 'ayurveda',\n",
       " 'babbitt',\n",
       " 'babe',\n",
       " 'baboon',\n",
       " 'baby',\n",
       " 'baca',\n",
       " 'back',\n",
       " 'backdrop',\n",
       " 'backed',\n",
       " 'background',\n",
       " 'backseat',\n",
       " 'backstage',\n",
       " 'backstory',\n",
       " 'backward',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'badder',\n",
       " 'badge',\n",
       " 'badly',\n",
       " 'baffled',\n",
       " 'baffling',\n",
       " 'bag',\n",
       " 'bagatelle',\n",
       " 'baio',\n",
       " 'bait',\n",
       " 'baked',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'bale',\n",
       " 'ball',\n",
       " 'ballast',\n",
       " 'ballerina',\n",
       " 'ballet',\n",
       " 'balletic',\n",
       " 'ballistic',\n",
       " 'balloon',\n",
       " 'ballot',\n",
       " 'balm',\n",
       " 'bam',\n",
       " 'banal',\n",
       " 'band',\n",
       " 'bang',\n",
       " 'banged',\n",
       " 'bar',\n",
       " 'baran',\n",
       " 'barbara',\n",
       " 'barbarism',\n",
       " 'barbera',\n",
       " 'bard',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'barker',\n",
       " 'barlow',\n",
       " 'barney',\n",
       " 'barrage',\n",
       " 'barrie',\n",
       " 'barrier',\n",
       " 'barris',\n",
       " 'barry',\n",
       " 'barrymore',\n",
       " 'bartleby',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'basement',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'bastard',\n",
       " 'bastion',\n",
       " 'bath',\n",
       " 'bathing',\n",
       " 'bathos',\n",
       " 'battista',\n",
       " 'battle',\n",
       " 'battlefield',\n",
       " 'battling',\n",
       " 'bawdy',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beachcombing',\n",
       " 'beacon',\n",
       " 'bean',\n",
       " 'bear',\n",
       " 'bearable',\n",
       " 'bearing',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beaten',\n",
       " 'beating',\n",
       " 'beatnik',\n",
       " 'beatrice',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'became',\n",
       " 'beck',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'bedroom',\n",
       " 'bedside',\n",
       " 'bedtime',\n",
       " 'bee',\n",
       " 'been',\n",
       " 'beer',\n",
       " 'befits',\n",
       " 'befuddled',\n",
       " 'befuddlement',\n",
       " 'began',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'begs',\n",
       " 'beguiling',\n",
       " 'begun',\n",
       " 'behan',\n",
       " 'behaved',\n",
       " 'behavior',\n",
       " 'behaviour',\n",
       " 'behind',\n",
       " 'behold',\n",
       " 'beijing',\n",
       " 'belated',\n",
       " 'belief',\n",
       " 'believability',\n",
       " 'believable',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'believer',\n",
       " 'believing',\n",
       " 'bella',\n",
       " 'belly',\n",
       " 'belo',\n",
       " 'belongs',\n",
       " 'beloved',\n",
       " 'belt',\n",
       " 'belushi',\n",
       " 'ben',\n",
       " 'benchmark',\n",
       " 'beneath',\n",
       " 'benefit',\n",
       " 'benign',\n",
       " 'benigni',\n",
       " 'benshan',\n",
       " 'bent',\n",
       " 'bergmanesque',\n",
       " 'berkley',\n",
       " 'berlin',\n",
       " 'bernal',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'betrayal',\n",
       " 'bettany',\n",
       " 'better',\n",
       " 'betty',\n",
       " 'bewildered',\n",
       " 'bewilderingly',\n",
       " 'bewitched',\n",
       " 'beyond',\n",
       " 'biased',\n",
       " 'bibi',\n",
       " 'bible',\n",
       " 'bicycle',\n",
       " 'bid',\n",
       " 'bidder',\n",
       " 'bielinsky',\n",
       " 'bien',\n",
       " 'bierbichler',\n",
       " 'big',\n",
       " 'bigelow',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bike',\n",
       " 'bile',\n",
       " 'bilked',\n",
       " 'bill',\n",
       " 'billy',\n",
       " 'bind',\n",
       " 'binks',\n",
       " 'binoche',\n",
       " 'bio',\n",
       " 'biopic',\n",
       " 'bird',\n",
       " 'birot',\n",
       " 'birthday',\n",
       " 'bisexual',\n",
       " 'bisset',\n",
       " 'bit',\n",
       " 'bite',\n",
       " 'biting',\n",
       " 'bitten',\n",
       " 'bitter',\n",
       " 'bittersweet',\n",
       " 'biz',\n",
       " 'bizarre',\n",
       " 'bjarne',\n",
       " 'bjorkness',\n",
       " 'black',\n",
       " 'blade',\n",
       " 'blair',\n",
       " 'blame',\n",
       " 'bland',\n",
       " 'blandly',\n",
       " 'blank',\n",
       " 'blanket',\n",
       " 'blaring',\n",
       " 'blast',\n",
       " 'blatant',\n",
       " 'blazing',\n",
       " 'bleak',\n",
       " 'bleakly',\n",
       " 'bled',\n",
       " 'bleibtreu',\n",
       " 'blend',\n",
       " 'blended',\n",
       " 'blender',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'bleu',\n",
       " 'blimey',\n",
       " 'blimp',\n",
       " 'blisteringly',\n",
       " 'blockbuster',\n",
       " 'blonde',\n",
       " 'blood',\n",
       " 'blooded',\n",
       " 'bloodshed',\n",
       " 'bloodstream',\n",
       " 'bloodsucker',\n",
       " 'bloody',\n",
       " 'blow',\n",
       " 'blowing',\n",
       " 'blown',\n",
       " 'blowout',\n",
       " 'blue',\n",
       " 'blueprint',\n",
       " 'bluescreen',\n",
       " 'bluff',\n",
       " 'blurry',\n",
       " 'blush',\n",
       " 'bmws',\n",
       " 'bmx',\n",
       " 'board',\n",
       " 'boarder',\n",
       " 'boast',\n",
       " 'boasting',\n",
       " 'boat',\n",
       " 'bob',\n",
       " 'bodice',\n",
       " 'bodied',\n",
       " 'bodily',\n",
       " 'body',\n",
       " 'bog',\n",
       " 'bogdanich',\n",
       " 'bogdanovich',\n",
       " 'bogged',\n",
       " 'bogus',\n",
       " 'boiling',\n",
       " 'boisterous',\n",
       " 'bold',\n",
       " 'bolder',\n",
       " 'boldly',\n",
       " 'bollywood',\n",
       " 'bolstered',\n",
       " 'bomb',\n",
       " 'bombay',\n",
       " 'bombing',\n",
       " 'bona',\n",
       " 'bond',\n",
       " 'bone',\n",
       " 'boob',\n",
       " 'book',\n",
       " 'boom',\n",
       " 'boomer',\n",
       " 'boorishness',\n",
       " 'boosted',\n",
       " 'boot',\n",
       " 'booty',\n",
       " 'boozy',\n",
       " 'border',\n",
       " 'bordering',\n",
       " 'borderline',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'boredom',\n",
       " 'boring',\n",
       " 'born',\n",
       " 'borrowed',\n",
       " 'borstal',\n",
       " 'botched',\n",
       " 'bother',\n",
       " 'bothered',\n",
       " 'bothersome',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'bouncy',\n",
       " 'bound',\n",
       " 'boundary',\n",
       " 'boundless',\n",
       " 'bourne',\n",
       " 'bout',\n",
       " 'bow',\n",
       " 'bowel',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boxing',\n",
       " 'boy',\n",
       " 'boyd',\n",
       " 'brady',\n",
       " 'brain',\n",
       " 'brainless',\n",
       " 'branagh',\n",
       " 'brand',\n",
       " 'brash',\n",
       " 'brass',\n",
       " 'brat',\n",
       " 'bratt',\n",
       " 'bratty',\n",
       " 'bravado',\n",
       " 'braveheart',\n",
       " 'bravely',\n",
       " 'bravery',\n",
       " 'bravo',\n",
       " 'bravura',\n",
       " 'brazenly',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breakdown',\n",
       " 'breaking',\n",
       " 'breakingly',\n",
       " 'breakthrough',\n",
       " 'breath',\n",
       " 'breathes',\n",
       " 'breathing',\n",
       " 'breathless',\n",
       " 'breathtaking',\n",
       " 'breathtakingly',\n",
       " 'brecht',\n",
       " 'breckin',\n",
       " 'breezy',\n",
       " 'breitbart',\n",
       " 'brendan',\n",
       " 'brew',\n",
       " 'brian',\n",
       " 'bride',\n",
       " 'bridge',\n",
       " 'bridget',\n",
       " 'brief',\n",
       " 'bright',\n",
       " 'brightly',\n",
       " 'brilliance',\n",
       " 'brilliant',\n",
       " 'brilliantly',\n",
       " 'brim',\n",
       " 'brimming',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'brink',\n",
       " 'brio',\n",
       " 'brisk',\n",
       " 'british',\n",
       " 'britney',\n",
       " 'broad',\n",
       " 'broadside',\n",
       " 'broadway',\n",
       " 'brogue',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brooding',\n",
       " 'brooklyn',\n",
       " 'broomfield',\n",
       " 'bros',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'bruce',\n",
       " 'bruising',\n",
       " 'brunt',\n",
       " 'brush',\n",
       " 'brushed',\n",
       " 'brutal',\n",
       " 'brutality',\n",
       " 'brutally',\n",
       " 'bryan',\n",
       " 'bubble',\n",
       " 'bubbly',\n",
       " 'bucket',\n",
       " 'budding',\n",
       " 'buddy',\n",
       " 'budget',\n",
       " 'buff',\n",
       " 'buffeted',\n",
       " 'buffoon',\n",
       " 'buggy',\n",
       " 'bui',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'bullet',\n",
       " 'bullfighter',\n",
       " 'bullock',\n",
       " 'bullwinkle',\n",
       " 'bumbling',\n",
       " ...]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = len(vocab)\n",
    "word_scores = np.zeros((V, 2))\n",
    "for i in range(V):\n",
    "    x_word = np.zeros((1, V))\n",
    "    x_word[:,i] = 1 # only the \"i\" word appeared\n",
    "    word_scores[i] = model.predict_proba(x_word)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44490945, 0.55509055],\n",
       "       [0.63665779, 0.36334221],\n",
       "       [0.48037753, 0.51962247],\n",
       "       ...,\n",
       "       [0.50078706, 0.49921294],\n",
       "       [0.49980275, 0.50019725],\n",
       "       [0.57915321, 0.42084679]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8541"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_neg = np.sort(word_scores.view('float64,float64'), order=['f1'], axis=0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.8560070368633268, 0.1439929631366732)],\n",
       " [(0.8385242792610248, 0.1614757207389752)],\n",
       " [(0.8362318540583138, 0.16376814594168615)],\n",
       " [(0.8334846032745392, 0.1665153967254608)],\n",
       " [(0.8328677501230323, 0.1671322498769677)]]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_neg.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_pos = np.sort(word_scores.view('float64,float64'), order=['f0'], axis=0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.1821982733787374, 0.8178017266212626)],\n",
       " [(0.23727185614495638, 0.7627281438550436)],\n",
       " [(0.23791880136993304, 0.762081198630067)],\n",
       " [(0.24410715666922478, 0.7558928433307752)],\n",
       " [(0.24675746897918704, 0.753242531020813)]]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_pos.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1334"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_scores.tolist().index([0.8560070368633268, 0.1439929631366732])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "for values in five_pos.tolist():\n",
    "    ind = word_scores.tolist().index(list(values[0]))\n",
    "    pos_words.append(vocab[ind])\n",
    "    \n",
    "for values in five_neg.tolist():\n",
    "    ind = word_scores.tolist().index(list(values[0]))\n",
    "    neg_words.append(vocab[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['world', 'solid', 'heart', 'rare', 'moving']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cliche', 'worst', 'dull', 'bad', 'boring']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index(\"awesome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38742827, 0.61257173])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_scores[585]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ambos casos se optó por obtener el top 5 y tanto en las palabras positivas como negativas obtenidas se observa que a nivel semántico no parecen palabras con una intensidad muy grande en sus respectivos sentimiento, por ejemplo la palabra \"world\" que viene siendo la mas positiva dentro de su conjunto, no es una palabra que denote mucho positivismo si se compara con \"good\", \"beautiful\" o \"awesome\". Esto da para pensar que de forma individual el modelo de regresion logistica con parametro de regularizacion C=1.0 puede no ser adecuado para realizar el trabajo, aun cuando al analizar frases tenga el mejor desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n)<a class=\"anchor\" id=\"n\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def base_word(word):\n",
    "    wordstemmer = PorterStemmer()\n",
    "    return wordstemmer.stem(word)\n",
    "def word_extractor(text):\n",
    "    wordstemmer = PorterStemmer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text) #substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ base_word(word.lower()) for word in word_tokenize(text) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords: #delete stopwords\n",
    "            words+=\" \"+word\n",
    "        \n",
    "    return words\n",
    "... #try yourself\n",
    "word_extractor(\"I love to eat cake\")\n",
    "word_extractor(\"I love eating cake\")\n",
    "word_extractor(\"I loved eating the cake\")\n",
    "word_extractor(\"I do not love eating cake\")\n",
    "word_extractor(\"I don't love eating cake\")\n",
    "... #try yourself\n",
    "texts_train = [word_extractor(text[1]) for text in df_train_text.to_numpy()]\n",
    "texts_val = [word_extractor(text[1]) for text in df_val_text.to_numpy()]\n",
    "texts_test = [word_extractor(text[1]) for text in df_test.to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary=False) #TF representation\n",
    "vectorizer.fit(texts_train)\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "... #transform val and test\n",
    "\n",
    "\n",
    "vectorizer2 = CountVectorizer(ngram_range=(1, 1), binary=False)\n",
    "vectorizer2.fit(texts_val)\n",
    "features_val = vectorizer.transform(texts_val)\n",
    "\n",
    "vectorizer3 = CountVectorizer(ngram_range=(1, 1), binary=False)\n",
    "vectorizer3.fit(texts_test)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "\n",
    "vocab2 = vectorizer.get_feature_names()\n",
    "dist2=list(np.array(features_val.sum(axis=0)).reshape(-1,))\n",
    "\n",
    "vocab3 = vectorizer.get_feature_names()\n",
    "dist3=list(np.array(features_test.sum(axis=0)).reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param C=  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model, train_acc, test_acc = do_LOGIT(features_train,labels_train,features_val,labels_val, param= 1.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7259425998874508"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, al realizar el proceso de Stemming se consigue un accuracy mayor en el test set en el regresor logistico con parametro de regularizacion C= 1.0. El aumento es de un 1% respecto al obtenido en el apartado [k](#k). Esto resulta curioso considerando que lemmatization considera el contexto a la hora de realizar la reduccion a la raiz mientras que stemming lo realiza considerando las palabras de forma individual, por lo que se pensaria inicialmente que utilizando el primero se obtendrian mejores resultados. Ahora el uso de stemming produce que al reducir una palabra en contextos diferentes se obtenga el mismo resultado, disminuyendo asi la cantidad de palabras que el TF consideraria, pudiendo ser esta la causa de un mejor desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o)<a class=\"anchor\" id=\"o\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_model = TfidfVectorizer(binary=False, ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, norm='l2', use_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfidf_model.fit(texts_train)\n",
    "features_train = tfidf_model.transform(texts_train)\n",
    "\n",
    "#tfidf_model.fit(texts_val)\n",
    "features_val = tfidf_model.transform(texts_val)\n",
    "\n",
    "#tfidf_model.fit(texts_test)\n",
    "features_test = tfidf_model.transform(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param C=  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chris\\pyenviroments\\machinelearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model, train_acc, test_acc = do_LOGIT(features_train,labels_train,features_val,labels_val, param= 1.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9331691874780161"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6947960618846695"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7386043894203714"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando TF-IDF se obtienen mejores resultados con el modelo de regresion logistica, aumentando el accuracy nuevamente llegando asi a un 74%. Se testeo cambiando parametros en el vecotirzador para utilizar unigramas y bigramas, y si bien el accuracy en training y en validacion suben, en testing disminuyen, por lo que se decide optar por solo dejar el uso de unigramas. Tambien se probo utilizando solo bigramas, pero el resultado de test set disminuyo por debajo del 60%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p)<a class=\"anchor\" id=\"p\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Analysis Testing Results ...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.74      0.74      0.74      1803\n",
      "           +       0.73      0.74      0.73      1751\n",
      "\n",
      "    accuracy                           0.74      3554\n",
      "   macro avg       0.74      0.74      0.74      3554\n",
      "weighted avg       0.74      0.74      0.74      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def score_the_model(model, x, y):\n",
    "    print(\"Detailed Analysis Testing Results ...\")\n",
    "    print(classification_report(y, model.predict(x), target_names=['-','+']))\n",
    "score_the_model(model, features_test, labels_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
