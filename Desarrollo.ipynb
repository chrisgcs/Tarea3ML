{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas:\n",
    "* [a](#a)\n",
    "* [b](#b)\n",
    "* [c](#c)\n",
    "* [d](#d)\n",
    "* [e](#e)\n",
    "* [f](#f)\n",
    "* [g](#g)\n",
    "* [h](#h)\n",
    "* [i](#i)\n",
    "* [j](#j)\n",
    "* [k](#k)\n",
    "* [l](#l)\n",
    "* [m](#m)\n",
    "* [n](#n)\n",
    "* [o](#o)\n",
    "* [p](#p)\n",
    "* [q](#p)\n",
    "* [r](#p)\n",
    "* [s](#s)\n",
    "* [t](#t)\n",
    "* [u](#u)\n",
    "* [v](#v)\n",
    "* [Actividad 2](#Actividad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ftr = open(\"train_data.csv\", \"r\",  encoding=\"ISO-8859-1\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "df_train = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "df_train['Sentiment'] = (pd.to_numeric(df_train['Sentiment'])+1)/2 # 0 o 1\n",
    "\n",
    "fts = open(\"test_data.csv\", \"r\",  encoding=\"ISO-8859-1\")\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "df_test = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "df_test['Sentiment'] = (pd.to_numeric(df_test['Sentiment'])+1)/2 # 0 o 1\n",
    "\n",
    "df_train_text = df_train.Text\n",
    "df_test_text = df_test.Text\n",
    "labels_train = df_train.Sentiment.values\n",
    "labels_test = df_test.Sentiment.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)<a class=\"anchor\" id=\"a\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3554 entries, 0 to 3553\n",
      "Data columns (total 2 columns):\n",
      "Sentiment    3554 non-null float64\n",
      "Text         3554 non-null object\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 55.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>everything's serious , poetic , earnest and --...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>narratively , trouble every day is a plodding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a truly wonderful tale combined with stunning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>jason patric and ray liotta make for one splen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>haneke keeps us at arm's length . guided more ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0        0.0  everything's serious , poetic , earnest and --...\n",
       "1        0.0  narratively , trouble every day is a plodding ...\n",
       "2        1.0  a truly wonderful tale combined with stunning ...\n",
       "3        1.0  jason patric and ray liotta make for one splen...\n",
       "4        0.0  haneke keeps us at arm's length . guided more ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "suma = 0\n",
    "i = 0\n",
    "for sentence in df_train['Text']:\n",
    "    if i == 0:\n",
    "        minimo = len(sentence)\n",
    "        maximo = len(sentence)\n",
    "    else:\n",
    "        if minimo>len(sentence):\n",
    "            minimo = len(sentence)\n",
    "            min_sent = sentence\n",
    "        if maximo<len(sentence):\n",
    "            maximo = len(sentence)\n",
    "            max_sent = sentence\n",
    "    i+=1\n",
    "    suma += len(sentence)\n",
    "avg_len = suma/len(df_train['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114.70709060213844, 7, 267, 'crummy\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(avg_len, minimo, maximo, min_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>screenwriter dan schneider and director shawn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>home alone goes hollywood , a funny premise un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>seldom has a movie so closely matched the spir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>less dizzying than just dizzy , the jaunt is p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>an ultra-low-budget indie debut that smacks mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0        0.0  screenwriter dan schneider and director shawn ...\n",
       "1        0.0  home alone goes hollywood , a funny premise un...\n",
       "2        1.0  seldom has a movie so closely matched the spir...\n",
       "3        0.0  less dizzying than just dizzy , the jaunt is p...\n",
       "4        0.0  an ultra-low-budget indie debut that smacks mo..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "suma = 0\n",
    "i = 0\n",
    "for sentence in df_test['Text']:\n",
    "    if i == 0:\n",
    "        minimo = len(sentence)\n",
    "        maximo = len(sentence)\n",
    "    else:\n",
    "        if minimo>len(sentence):\n",
    "            minimo = len(sentence)\n",
    "            min_sent = sentence\n",
    "        if maximo<len(sentence):\n",
    "            maximo = len(sentence)\n",
    "            max_sent = sentence\n",
    "    i+=1\n",
    "    suma += len(sentence)\n",
    "avg_len = suma/len(df_train['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116.4679234665166, 9, 268, 'horrible\\n')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(avg_len, minimo, maximo, min_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un mejor analisis preliminar seria de utilidad la eliminacion de las stopwords a la hora de poder determinar los largos maximo, minimo y el promedio del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)<a class=\"anchor\" id=\"b\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train_text, df_val_text, labels_train, labels_val  = train_test_split(df_train, labels_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2843, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(711, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c)<a class=\"anchor\" id=\"c\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "def base_word(word):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    return wordlemmatizer.lemmatize(word) \n",
    "def word_extractor(text):\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text) #substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ base_word(word.lower()) for word in word_tokenize(text) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords: #delete stopwords\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "... #try yourself\n",
    "word_extractor(\"I love to eat cake\")\n",
    "word_extractor(\"I love eating cake\")\n",
    "word_extractor(\"I loved eating the cake\")\n",
    "word_extractor(\"I do not love eating cake\")\n",
    "word_extractor(\"I don't love eating cake\")\n",
    "... #try yourself\n",
    "texts_train = [word_extractor(text[1]) for text in df_train_text.to_numpy()]\n",
    "texts_val = [word_extractor(text[1]) for text in df_val_text.to_numpy()]\n",
    "texts_test = [word_extractor(text[1]) for text in df_test.to_numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La importancia del preprocesamiento en el dominio del lenguaje natural radica en disminuir las palabras que si bien aparecen con mas frecuencia son las menos significantes para realizar analisis (las stopwords) para asi reducir ruido y tiempo de ejecucion. Tambien es importante el proceso de reducir las palabras a su origen con stemming o lematizacion para evitar analizar palabras cuya semantica es identica pero estan escritas diferentes producto de una conjugacion. En este caso se utiliza lematizacion ya que presenta mejores resultados que stemming al realizar el proceso considerando el contexto en el cual se encuentra la palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d)<a class=\"anchor\" id=\"d\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary=False) #TF representation\n",
    "vectorizer.fit(texts_train)\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "... #transform val and test\n",
    "\n",
    "\n",
    "vectorizer2 = CountVectorizer(ngram_range=(1, 1), binary=False)\n",
    "vectorizer2.fit(texts_val)\n",
    "features_val = vectorizer.transform(texts_val)\n",
    "\n",
    "vectorizer3 = CountVectorizer(ngram_range=(1, 1), binary=False)\n",
    "vectorizer3.fit(texts_test)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "\n",
    "vocab2 = vectorizer.get_feature_names()\n",
    "dist2=list(np.array(features_val.sum(axis=0)).reshape(-1,))\n",
    "\n",
    "vocab3 = vectorizer.get_feature_names()\n",
    "dist3=list(np.array(features_test.sum(axis=0)).reshape(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e)<a class=\"anchor\" id=\"e\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "for s in labels_train:\n",
    "    if s == 1:\n",
    "        sentiment.append(\"Positivo\")\n",
    "    else:\n",
    "        sentiment.append(\"Negativo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD #aka LSA\n",
    "import matplotlib.pyplot as plt\n",
    "model = TruncatedSVD(n_components=2)\n",
    "model.fit(features_train)\n",
    "x_plot = model.transform(features_train)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(x_plot[:,0], x_plot[:,1], c=labels_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1786], dtype=int64), array([0], dtype=int64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(x_plot == x_plot.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.95015737, 1.75218186])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_plot[1786]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positivo'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment[1786]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amarillo Positivo, Morado Negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las agrupaciones que se forman parecen contener una cantidad equitativa de positivo y negativo. Estas agrupaciones corresponden a las tematicas de las distintas criticas, pudiendo ser que cada agrupacion sea una pelicula distinta o que hablen sobre caracteristicas de las peliculas como la musica, los actores, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f)<a class=\"anchor\" id=\"f\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d63442b4d34a5291c669c0528885b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Css', options=(0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interact_doLOGIT(Css)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "\n",
    "def do_LOGIT(x,y,xv,yv, param):\n",
    "    print(\"Param C= \",param)\n",
    "    model= LogisticRegression(penalty = 'l2')\n",
    "    model.set_params(C=param)\n",
    "    model.fit(x,y)\n",
    "    train_acc = model.score(x,y)\n",
    "    test_acc = model.score(xv,yv)\n",
    "    return model, train_acc, test_acc\n",
    "#Cs = [10**i for i in np.arange(-4,4)]\n",
    "Cs = np.power(np.repeat(10., 8),np.arange(-4,4))\n",
    "def interact_doLOGIT(Css):\n",
    "    model, train_acc, test_acc = do_LOGIT(features_train,labels_train,features_val,labels_val, param= Css )\n",
    "    print(\"train acc: \",train_acc)\n",
    "    print(\"test acc: \",test_acc)\n",
    "    \n",
    "    #Train\n",
    "    lista_train = model.predict(features_train)\n",
    "    #\n",
    "    y_true_train = list(map(str,lista_train))\n",
    "    y_pred_train = list(map(str,labels_train))\n",
    "    data = confusion_matrix(y_true_train, y_pred_train)\n",
    "    df_cm = pd.DataFrame(data, columns=np.unique(y_true_train), index = np.unique(y_true_train))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted Train'\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.set(font_scale=1.2)#for label size\n",
    "    sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14})# font size\n",
    "    \n",
    "    #Val\n",
    "    lista_val = model.predict(features_val)\n",
    "    #\n",
    "    y_true_val = list(map(str,lista_val))\n",
    "    y_pred_val = list(map(str,labels_val))\n",
    "    data = confusion_matrix(y_true_val, y_pred_val)\n",
    "    df_cm = pd.DataFrame(data, columns=np.unique(y_true_val), index = np.unique(y_true_val))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted Val'\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.set(font_scale=1.2)#for label size\n",
    "    sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14})# font size\n",
    "    \n",
    "interact(interact_doLOGIT, Css=Cs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones de lo que se observa de los heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g)<a class=\"anchor\" id=\"g\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
